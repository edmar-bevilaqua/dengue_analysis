{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e421ec",
   "metadata": {},
   "source": [
    "# Dengue Analysis:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import (\n",
    "    print_with_colors,\n",
    "    is_int,\n",
    "    process_num_like_cols,\n",
    "    print_with_multiple_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b904ae",
   "metadata": {},
   "source": [
    "## Download:\n",
    "---\n",
    "* Download the Dataset from Web if not already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./raw_data\"):\n",
    "    os.makedirs(\"./raw_data\")\n",
    "\n",
    "if not os.path.exists(\"./raw_data/arbovirus_clinical_data\"):\n",
    "    # Download of .zip file\n",
    "    url = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/2d3kr8zynf-4.zip\"\n",
    "    output = f\"./raw_data/dataset.zip\"\n",
    "    subprocess.run([\"wget\", \"--quiet\", \"--no-check-certificate\", url, \"-O\", output])\n",
    "\n",
    "    # Extraction of .zip file\n",
    "    subprocess.run([\"unzip\", output])\n",
    "    subprocess.run([\"mv\", \"./2d3kr8zynf-4\", \"./raw_data/arbovirus_clinical_data\"])\n",
    "    subprocess.run([\"rm\", output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca0fc1",
   "metadata": {},
   "source": [
    "## Reading the Dataset:\n",
    "---\n",
    "* Using `chunksize` on `pd.read_csv()` method to use less RAM memory during reading\n",
    "\n",
    "<font color='yellow'>Note: if you already have the .parquet file, you can skip to [this](#file) section<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19baebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "missing_values = [\n",
    "    '', ' ', 'NA', 'N/A', 'NULL',\n",
    "    'ID_AGRAVO', 'DT_NOTIFIC', 'SEM_NOT', 'NU_ANO', 'SG_UF_NOT',\n",
    "    'ID_MUNICIP', 'ID_REGIONA', 'ID_UNIDADE', 'DT_SIN_PRI', 'SEM_PRI',\n",
    "    'DT_NASC', 'NU_IDADE_N', 'CS_SEXO', 'CS_GESTANT', 'CS_RACA',\n",
    "    'CS_ESCOL_N', 'SG_UF', 'ID_MN_RESI', 'ID_RG_RESI', 'ID_PAIS',\n",
    "    'DT_INVEST', 'FEBRE', 'MIALGIA', 'CEFALEIA', 'EXANTEMA',\n",
    "    'VOMITO', 'NAUSEA', 'DOR_COSTAS', 'CONJUNTVIT', 'ARTRITE',\n",
    "    'ARTRALGIA', 'PETEQUIA_N', 'LEUCOPENIA', 'LACO', 'DOR_RETRO',\n",
    "    'DIABETES', 'HEMATOLOG', 'HEPATOPAT', 'RENAL', 'HIPERTENSA',\n",
    "    'ACIDO_PEPT', 'AUTO_IMUNE', 'RESUL_SORO', 'RESUL_NS1', 'RESUL_VI_N',\n",
    "    'RESUL_PCR_', 'HISTOPA_N', 'IMUNOH_N', 'HOSPITALIZ', 'TPAUTOCTO',\n",
    "    'COUFINF', 'COPAISINF', 'COMUNINF', 'CLASSI_FIN', 'EVOLUCAO', 'DT_ENCERRA', '.'\n",
    "]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Low memory safe reading of the CSV file\n",
    "splitted_df = pd.read_csv(\n",
    "    './raw_data/arbovirus_clinical_data/dengue.csv',\n",
    "    sep=',',\n",
    "    header=0,\n",
    "    na_values=missing_values,\n",
    "    chunksize=100_000,\n",
    ")\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dengue_df = pd.concat(splitted_df, ignore_index=True)\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ad20d",
   "metadata": {},
   "source": [
    "* The file `attributes.csv` has important information about the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(\"raw_data/arbovirus_clinical_data/attributes.csv\", sep=\",\", header=0, low_memory=False)\n",
    "attributes = attributes.ffill()\n",
    "attributes = attributes.groupby([\"Attribute\", \"Description\"])[\"Value\"].apply('; '.join).reset_index(name=\"Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df4c0d",
   "metadata": {},
   "source": [
    "## Pre Processing\n",
    "---\n",
    "### Null Data Removal:\n",
    "* Features with frequency > 60% of null values are dropped.\n",
    "* Also, columns like `[\"CS_FLXRET\", \"TP_SISTEMA\", \"CRITERIO\", \"TP_NOT\", \"Unnamed: 0\"]` doesn't have useful information, therefore they can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_df = dengue_df.loc[:, dengue_df.isnull().mean() < .60]\n",
    "dengue_df = dengue_df.drop(columns=[\"CS_FLXRET\", \"TP_SISTEMA\", \"CRITERIO\", \"TP_NOT\", \"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac3f37",
   "metadata": {},
   "source": [
    "* Printing unique values for each feature to check their data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a82172",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dengue_df.columns.to_list():\n",
    "    if str(col) in attributes[\"Attribute\"].to_list():\n",
    "        print(f\"Column '{col}' has {dengue_df[col].unique().size} unique values.\")\n",
    "        if dengue_df[col].unique().size < 50:\n",
    "            print(dengue_df[col].unique(), end=\"\\n\\n\")\n",
    "        else:\n",
    "            print(\"To many unique values, skipping...\", end=\"\\n\\n\")\n",
    "    else:\n",
    "        print_with_colors(f\"Column '{col}' not in attributes. Skipping display...\", \"yellow\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18200c2",
   "metadata": {},
   "source": [
    "### Standardization of column values:\n",
    "* Since the system has changed over the year, multiple codes were used to represent some types of Dengue. In the cell below we standardized these problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_df['CLASSI_FIN'] = dengue_df['CLASSI_FIN'].astype('object')\n",
    "\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==1, 'CLASSI_FIN'] = 'Dengue'\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==10, 'CLASSI_FIN'] = 'Dengue'\n",
    "\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==3, 'CLASSI_FIN'] = 'Dengue Grave'\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==4, 'CLASSI_FIN'] = 'Dengue Grave'\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==12, 'CLASSI_FIN'] = 'Dengue Grave'\n",
    "\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==2, 'CLASSI_FIN'] = 'Dengue com sinais de alarme'\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==11, 'CLASSI_FIN'] = 'Dengue com sinais de alarme'\n",
    "\n",
    "# Discarded/Inconclusive\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==5, 'CLASSI_FIN'] = 'Discarded/Inconclusive'\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==6, 'CLASSI_FIN'] = 'Discarded/Inconclusive'\n",
    "dengue_df.loc[dengue_df['CLASSI_FIN']==8, 'CLASSI_FIN'] = 'Discarded/Inconclusive'\n",
    "\n",
    "dengue_df['CLASSI_FIN'] = dengue_df['CLASSI_FIN'].fillna('Discarded/Inconclusive')\n",
    "dengue_df['CLASSI_FIN'] = dengue_df['CLASSI_FIN'].astype('category')\n",
    "\n",
    "\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: x-4000 if (x >= 4000 and x <= 4999) else x)\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: x-400 if (x >= 400 and x <= 499) else x)\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: 1 if (x >= 3000 and x <= 3999) else x)\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: 1 if (x >= 300 and x <= 399) else x)\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: 1 if (x >= 2000 and x <= 2999) else x)\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: 1 if (x >= 200 and x <= 299) else x)\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: 1 if (x >= 1000 and x <= 1999) else x)\n",
    "dengue_df[\"NU_IDADE_N\"] = dengue_df[\"NU_IDADE_N\"].apply(lambda x: 1 if (x >= 100 and x <= 199) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66454f7",
   "metadata": {},
   "source": [
    "### Null data padding with default values:\n",
    "The resulting attributes that still had null data were entered with the default values referring to the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_cols = [\n",
    "    \"RESUL_SORO\",\n",
    "    \"RESUL_NS1\",\n",
    "    \"RESUL_VI_N\",\n",
    "    \"RESUL_PCR_\",\n",
    "    \"HISTOPA_N\",\n",
    "    \"IMUNOH_N\"\n",
    "]\n",
    "for col in exam_cols:\n",
    "    if dengue_df[col].isnull().sum() > 0:\n",
    "        dengue_df.loc[dengue_df[col].isnull(), col] = 4\n",
    "\n",
    "dengue_df['CS_SEXO'] = dengue_df['CS_SEXO'].fillna('I')\n",
    "\n",
    "# In the other attributes, the value of \"not informed\" is 9.\n",
    "columns_to_be_filled = [\n",
    "    col\n",
    "    for col in dengue_df.columns\n",
    "    if col not in exam_cols\n",
    "    and 'DT_' not in str(col) # for datetime columns it doesn't make sense\n",
    "    and not 'CS_SEXO'.__eq__(str(col)) # CS_SEXO has the special value 'I' for NaNs\n",
    "]\n",
    "for col in columns_to_be_filled:\n",
    "    if dengue_df[col].isnull().sum() > 0:\n",
    "        dengue_df.loc[dengue_df[col].isnull(), col] = 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b83ee5",
   "metadata": {},
   "source": [
    "Removing columns 'ID_AGRAVO', because it has only 1 fixed value: `A90`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ec960",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_df = dengue_df.drop(columns=['ID_AGRAVO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14aa0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'SEM_NOT': 'category',\n",
    "    'NU_ANO': 'int16',\n",
    "    'SG_UF_NOT': 'category',\n",
    "    'ID_MUNICIP': 'category',\n",
    "    'ID_REGIONA': 'category',\n",
    "    'ID_UNIDADE': 'category',\n",
    "    'SEM_PRI': 'int32',\n",
    "    'NU_IDADE_N': 'int8',\n",
    "    'CS_SEXO': 'category',\n",
    "    'CS_GESTANT': 'category',\n",
    "    'CS_RACA': 'category',\n",
    "    'CS_ESCOL_N': 'category',\n",
    "    'SG_UF': 'category',\n",
    "    'ID_MN_RESI': 'category',\n",
    "    'ID_RG_RESI': 'category',\n",
    "    'ID_PAIS': 'category',\n",
    "    'FEBRE': 'category',\n",
    "    'MIALGIA': 'category',\n",
    "    'CEFALEIA': 'category',\n",
    "    'EXANTEMA': 'category',\n",
    "    'VOMITO': 'category',\n",
    "    'NAUSEA': 'category',\n",
    "    'DOR_COSTAS': 'category',\n",
    "    'CONJUNTVIT': 'category',\n",
    "    'ARTRITE': 'category',\n",
    "    'ARTRALGIA': 'category',\n",
    "    'PETEQUIA_N': 'category',\n",
    "    'LEUCOPENIA': 'category',\n",
    "    'LACO': 'category',\n",
    "    'DOR_RETRO': 'category',\n",
    "    'DIABETES': 'category',\n",
    "    'HEMATOLOG': 'category',\n",
    "    'HEPATOPAT': 'category',\n",
    "    'RENAL': 'category',\n",
    "    'HIPERTENSA': 'category',\n",
    "    'ACIDO_PEPT': 'category',\n",
    "    'AUTO_IMUNE': 'category',\n",
    "    'RESUL_SORO': 'category',\n",
    "    'RESUL_NS1': 'category',\n",
    "    'RESUL_VI_N': 'category',\n",
    "    'RESUL_PCR_': 'category',\n",
    "    'HISTOPA_N': 'category',\n",
    "    'IMUNOH_N': 'category',\n",
    "    'HOSPITALIZ': 'category',\n",
    "    'TPAUTOCTO': 'category',\n",
    "    'COUFINF': 'category',\n",
    "    'COPAISINF': 'category',\n",
    "    'COMUNINF': 'category',\n",
    "    'EVOLUCAO': 'category',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_df = process_num_like_cols(dengue_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7487e4",
   "metadata": {},
   "source": [
    "### Setting dtypes to columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['DT_NOTIFIC', 'DT_SIN_PRI', 'DT_NASC', 'DT_INVEST', 'DT_ENCERRA']\n",
    "for col in date_cols:\n",
    "    if col in dengue_df.columns:\n",
    "        dengue_df[col] = pd.to_datetime(dengue_df[col], errors='coerce')\n",
    "\n",
    "dengue_df['SEM_PRI'] = dengue_df['SEM_PRI'].apply(lambda x: x.replace('-', '') if isinstance(x, str) else x)\n",
    "\n",
    "exam_cols = [\n",
    "    \"RESUL_SORO\",\n",
    "    \"RESUL_NS1\",\n",
    "    \"RESUL_VI_N\",\n",
    "    \"RESUL_PCR_\",\n",
    "    \"HISTOPA_N\",\n",
    "    \"IMUNOH_N\"\n",
    "]\n",
    "for col in exam_cols:\n",
    "    if dengue_df[col].isnull().sum() > 0:\n",
    "        dengue_df.loc[dengue_df[col].isnull(), col] = 4\n",
    "\n",
    "dengue_df['CS_SEXO'] = dengue_df['CS_SEXO'].fillna('I')\n",
    "\n",
    "# In the other attributes, the value of \"not informed\" is 9.\n",
    "columns_to_be_filled = [\n",
    "    col\n",
    "    for col in dengue_df.columns\n",
    "    if col not in exam_cols\n",
    "    and 'DT_' not in str(col) # for datetime columns it doesn't make sense\n",
    "    and not 'CS_SEXO'.__eq__(str(col)) # CS_SEXO has the special value 'I' for NaNs\n",
    "]\n",
    "for col in columns_to_be_filled:\n",
    "    if dengue_df[col].isnull().sum() > 0:\n",
    "        dengue_df.loc[dengue_df[col].isnull(), col] = 9\n",
    "\n",
    "dengue_df = dengue_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e8613",
   "metadata": {},
   "source": [
    "### Saving processed DataFrame to parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8584b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_df.to_parquet(\"./preprocessed_data/dengue.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c55045",
   "metadata": {},
   "source": [
    "## Reading Parquet File:\n",
    "---\n",
    "* Uncomment the cell below if you already have the `.parquet` file and just want to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dengue_df = pd.read_parquet(\"./preprocessed_data/dengue.parquet\")\n",
    "\n",
    "# dtypes = {\n",
    "#     'SEM_NOT': 'int32', 'NU_ANO': 'int16', 'SG_UF_NOT': 'category', 'ID_MUNICIP': 'category',\n",
    "#     'ID_REGIONA': 'category', 'ID_UNIDADE': 'category', 'SEM_PRI': 'int32',\n",
    "#     'NU_IDADE_N': 'int8', 'CS_SEXO': 'category', 'CS_GESTANT': 'category',\n",
    "#     'CS_RACA': 'category', 'CS_ESCOL_N': 'category', 'SG_UF': 'category',\n",
    "#     'ID_MN_RESI': 'category', 'ID_RG_RESI': 'category', 'ID_PAIS': 'category',\n",
    "#     'FEBRE': 'category', 'MIALGIA': 'category', 'CEFALEIA': 'category',\n",
    "#     'EXANTEMA': 'category', 'VOMITO': 'category', 'NAUSEA': 'category',\n",
    "#     'DOR_COSTAS': 'category', 'CONJUNTVIT': 'category', 'ARTRITE': 'category',\n",
    "#     'ARTRALGIA': 'category', 'PETEQUIA_N': 'category', 'LEUCOPENIA': 'category',\n",
    "#     'LACO': 'category', 'DOR_RETRO': 'category', 'DIABETES': 'category',\n",
    "#     'HEMATOLOG': 'category', 'HEPATOPAT': 'category', 'RENAL': 'category',\n",
    "#     'HIPERTENSA': 'category', 'ACIDO_PEPT': 'category', 'AUTO_IMUNE': 'category',\n",
    "#     'RESUL_SORO': 'category', 'RESUL_NS1': 'category', 'RESUL_VI_N': 'category',\n",
    "#     'RESUL_PCR_': 'category', 'HISTOPA_N': 'category', 'IMUNOH_N': 'category',\n",
    "#     'HOSPITALIZ': 'category', 'TPAUTOCTO': 'category', 'COUFINF': 'category',\n",
    "#     'COPAISINF': 'category', 'COMUNINF': 'category', 'EVOLUCAO': 'category',\n",
    "# }\n",
    "\n",
    "# dengue_df = dengue_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2d440",
   "metadata": {},
   "source": [
    "## Removing Data Leakage Features:\n",
    "---\n",
    "Some features in the dataset have information from the future (*i.e.* after the `CLASSI_FIN` has been diagnosed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8669d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_columns = [\n",
    "    'RESUL_SORO', 'RESUL_NS1', 'RESUL_VI_N', 'RESUL_PCR_', 'HISTOPA_N',\n",
    "    'IMUNOH_N', 'EVOLUCAO', 'DT_ENCERRA', 'TPAUTOCTO', 'COUFINF', 'COPAISINF',\n",
    "    'COMUNINF', 'CODISINF', 'CO_BAINFC', 'NOBAIINF', 'DOENCA_TRA', 'DT_OBITO',\n",
    "    \n",
    "]\n",
    "leaky_columns = [col for col in leaky_columns if col in dengue_df.columns]\n",
    "\n",
    "dengue_df = dengue_df.drop(columns=leaky_columns)\n",
    "\n",
    "dengue_df = dengue_df[~dengue_df[\"NU_IDADE_N\"] < 0] # Age < 0 don't make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778b8f3",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "---\n",
    "* Trying to extract useful information from features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with error: 'DT_SIN_PRI' cannot be equal to 'DT_NASC'.\n",
    "dengue_df = dengue_df[~(dengue_df['DT_NASC'] == dengue_df['DT_SIN_PRI'])]\n",
    "\n",
    "# Time gap between notification and first simptoms.\n",
    "dengue_df['time_until_report'] = (dengue_df['DT_NOTIFIC'] - dengue_df['DT_SIN_PRI']).dt.days\n",
    "\n",
    "# Negative values for 'time_until_report' or 'time_until_report' >= 30 doesn't make sense, since the virus\n",
    "# expresses their simptoms in a shorter period of time.\n",
    "dengue_df = dengue_df.loc[(dengue_df['time_until_report'] <= 30) & (dengue_df['time_until_report'] >= 0)]\n",
    "\n",
    "# 'SEM_NOT' is the epidemiological week of the notification date.\n",
    "# We don't need the year, so we can convert it to a string and remove the first characters.\n",
    "dengue_df['SEM_NOT'] = dengue_df['SEM_NOT'].astype('str')\n",
    "dengue_df['SEM_NOT'] = dengue_df['SEM_NOT'].apply(lambda x: x[4:] if len(x) > 4 else x[2:])\n",
    "dengue_df['SEM_NOT'] = dengue_df['SEM_NOT'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=dengue_df['time_until_report'], orient='horizontal')\n",
    "plt.title('Distribuição de \"time_until_report\"')\n",
    "plt.xlabel('Dias entre Sintomas e Notificação')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(\"raw_data/arbovirus_clinical_data/attributes.csv\", sep=\",\", header=0, low_memory=False)\n",
    "attributes = attributes.ffill()\n",
    "attributes = attributes.groupby([\"Attribute\", \"Description\"])[\"Value\"].apply('; '.join).reset_index(name=\"Values\")\n",
    "attributes = attributes[~attributes[\"Attribute\"].isin(leaky_columns)]\n",
    "attributes = attributes[attributes[\"Attribute\"].isin(dengue_df.columns)]\n",
    "attributes = attributes.reset_index(drop=True)\n",
    "\n",
    "acido_pept = {\n",
    "    \"Attribute\": \"ACIDO_PEPT\",\n",
    "    \"Description\": \"Pre-existing disease - Acid peptic disease\",\n",
    "    \"Values\": \"1: Yes; 2: No\",\n",
    "}\n",
    "\n",
    "time_until_report = {\n",
    "    \"Attribute\": \"time_until_report\",\n",
    "    \"Description\": \"Time until report\",\n",
    "    \"Values\": \"0: 0 days; 1: 1 day; 2: 2 days; ...; 30: 30 days\",\n",
    "}\n",
    "attributes = pd.concat([attributes, pd.DataFrame([acido_pept]), pd.DataFrame([time_until_report])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30903521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(dengue_df['CLASSI_FIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep mapping of target variable\n",
    "target_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(f\"Target mapping: {target_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78260c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the feature matrix\n",
    "X = dengue_df.drop(columns=['CLASSI_FIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting month and day of the week from the notification date\n",
    "if 'DT_NOTIFIC' in X.columns:\n",
    "    X['notif_month'] = X['DT_NOTIFIC'].dt.month\n",
    "    X['notif_week_day'] = X['DT_NOTIFIC'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove raw date columns\n",
    "cols_to_drop = [col for col in X.columns if 'DT_' in col]\n",
    "X = X.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features to train with:\")\n",
    "print_with_multiple_columns(X.columns.tolist(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-checking for 'object' columns and converting them to appropriate types\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = X[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4cab2f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning In a Small Subset:\n",
    "---\n",
    "* Instead of using 12 million registries, we will do Hyperparameter Tuning using only 500k samples, to save time and computer power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting optuna objective function\n",
    "def objective_parallel(trial, X_inner, y_inner):\n",
    "    \"\"\"\n",
    "    Versão otimizada para paralelismo. Cada trial usará apenas 1 core.\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_inner, y_inner, test_size=0.25, random_state=42, stratify=y_inner\n",
    "    )\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multiclass', 'metric': 'multi_logloss', 'num_class': len(np.unique(y_inner)),\n",
    "        'verbosity': -1, 'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'n_jobs': 1,\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params, n_estimators=1000)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(15, verbose=False)]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "    f1 = f1_score(y_val, preds, average='macro')\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on a small subset\n",
    "# Stratified sampling for training...\n",
    "dengue_df['strata'] = (\n",
    "    dengue_df['NU_ANO'].astype(str) + '_' +\n",
    "    dengue_df['CLASSI_FIN'].astype(str)\n",
    ")\n",
    "\n",
    "# We will use a large ~ 5% (500k) sample size to ensure we have enough data for training.\n",
    "sample_size = 500_000\n",
    "sample_ratio = sample_size / len(dengue_df)\n",
    "\n",
    "_, df_subset = train_test_split(\n",
    "    dengue_df,\n",
    "    test_size=sample_ratio,\n",
    "    stratify=dengue_df['strata'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Sample subset created with size:\", len(df_subset))\n",
    "\n",
    "df_subset = df_subset.drop(columns=['strata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_subset['CLASSI_FIN'])\n",
    "\n",
    "# Keep mapping of target variable\n",
    "target_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(f\"Target mapping: {target_mapping}\")\n",
    "\n",
    "# Preparing the feature matrix\n",
    "X = df_subset.drop(columns=['CLASSI_FIN'])\n",
    "\n",
    "# Extracting month and day of the week from the notification date\n",
    "if 'DT_NOTIFIC' in X.columns:\n",
    "    X['notif_month'] = X['DT_NOTIFIC'].dt.month\n",
    "    X['notif_week_day'] = X['DT_NOTIFIC'].dt.dayofweek\n",
    "    \n",
    "# Remove raw date columns\n",
    "cols_to_drop = [col for col in X.columns if 'DT_' in col]\n",
    "X = X.drop(columns=cols_to_drop)\n",
    "\n",
    "# Double-checking for 'object' columns and converting them to appropriate types\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = X[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e035041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested Cross Validation:\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=100)\n",
    "\n",
    "outer_scores = []\n",
    "best_params_per_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Nested Cross Validation...\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    print(f\"\\n--- Processing over external fold {i+1}/5 ---\")\n",
    "    \n",
    "    X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(\n",
    "        lambda trial: objective_parallel(trial, X_train_outer, y_train_outer),\n",
    "        n_trials=20,  # You can adjust the number of trials based on your computational resources\n",
    "        n_jobs=-1     # <-- Here we use all available cores for parallel execution\n",
    "    )\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_params['n_jobs'] = 1 \n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "    final_model = lgb.LGBMClassifier(**best_params, n_estimators=1000, random_state=42)\n",
    "    \n",
    "    final_model.fit(\n",
    "        X_train_outer, y_train_outer,\n",
    "        eval_set=[(X_test_outer, y_test_outer)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(15, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred_outer = final_model.predict(X_test_outer)\n",
    "    score = f1_score(y_test_outer, y_pred_outer, average='macro')\n",
    "    outer_scores.append(score)\n",
    "    print(f\"F1-Macro for External Fold: {i+1}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Evaluation of the Optimized Nested Cross-Validation ---\")\n",
    "print(f\"Macro F1-Scores for each outer fold: {np.round(outer_scores, 4)}\")\n",
    "print(f\"Mean Macro F1-Score: {np.mean(outer_scores):.4f}\")\n",
    "print(f\"Macro F1-Score Standard Deviation: {np.std(outer_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SIMPLIFY WITH A SINGLE SPLIT ---\n",
    "# We don't need Nested CV to find the leak\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- STEP 1: TRAIN A MODEL AND CHECK FEATURE IMPORTANCE ---\n",
    "print(\"Training a LightGBM model for importance analysis...\")\n",
    "simple_lgbm = lgb.LGBMClassifier(objective='multiclass', random_state=42)\n",
    "simple_lgbm.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- FEATURE IMPORTANCE PLOT (LGBM) ---\")\n",
    "print(\"Look for a bar that is MUCH taller than all the others.\")\n",
    "lgb.plot_importance(simple_lgbm, max_num_features=20, figsize=(10, 8),\n",
    "                    importance_type='gain', title='Feature Importance (Gain)')\n",
    "plt.show()\n",
    "\n",
    "# The feature name at the top of the plot is our SUSPECT #1.\n",
    "\n",
    "# --- STEP 2: VISUALIZE A SIMPLE DECISION TREE ---\n",
    "print(\"\\n--- SIMPLE DECISION TREE PLOT ---\")\n",
    "print(\"The feature at the top of the tree (root node) is the most likely culprit.\")\n",
    "# We need an X with only numeric columns for plot_tree\n",
    "X_train_numeric = X_train.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "simple_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "simple_tree.fit(X_train_numeric, y_train)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(simple_tree,\n",
    "          feature_names=X_train_numeric.columns,\n",
    "          class_names=le.classes_,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Simple Decision Tree to Identify the Dominant Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# TODO more tests are needed to check if the feature is really a leak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
